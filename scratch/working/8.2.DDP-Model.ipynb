{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 2.0] 모델 빌딩하기 (No VPC 환경에서 실행하세요)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (2.25.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (1.16.63)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (20.8)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (1.18.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker) (3.4.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.63 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.19.63)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.63->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.63->boto3>=1.16.32->sagemaker) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Use SageMaker local for local testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sagemaker.tensorflow` 클래스를 사용하여 SageMaker Python SDK의 Tensorflow Estimator 인스턴스를 생성합니다.\n",
    "인자값으로 하이퍼파라메터와 다양한 설정들을 변경할 수 있습니다.\n",
    "\n",
    "자세한 내용은 [documentation](https://sagemaker.readthedocs.io/en/stable/using_tf.html#training-with-tensorflow-estimator)을 확인하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-2-057716757052/data/DEMO-cifar10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-22 13:59:14 Starting - Starting the training job...\n",
      "2021-02-22 13:59:36 Starting - Launching requested ML instancesProfilerReport-1614002353: InProgress\n",
      ".........\n",
      "2021-02-22 14:00:58 Starting - Preparing the instances for training............\n",
      "2021-02-22 14:03:09 Downloading - Downloading input data\n",
      "2021-02-22 14:03:09 Training - Downloading the training image...............\n",
      "2021-02-22 14:05:41 Training - Training image download completed. Training in progress.\u001b[34m2021-02-22 14:05:31.763971: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:31.768381: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:32.006951: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:32.101691: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:35,414 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:36,276 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:36,277 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:36,288 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:36,288 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:36,288 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:36,374 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-cifar10-ddp-2021-02-22-13-59-13-709\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_keras_sm_ddp_82\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_keras_sm_ddp_82.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_keras_sm_ddp_82.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_keras_sm_ddp_82\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-cifar10-ddp-2021-02-22-13-59-13-709\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_keras_sm_ddp_82\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_keras_sm_ddp_82.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so smddprun /usr/local/bin/python3.7 -m mpi4py cifar10_keras_sm_ddp_82.py --model_dir s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.79.111<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO comm 0x559dae907180 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO comm 0x5643a7ee7980 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO comm 0x55f3390b45b0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO comm 0x557fd841dd80 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO comm 0x559267c37d40 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO comm 0x55fd77472640 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO comm 0x555804925310 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO comm 0x5558448cdb00 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:156:156 [1] NCCL INFO comm 0x559db15d8260 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:154:154 [0] NCCL INFO comm 0x5643aabb8a60 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:158:158 [2] NCCL INFO comm 0x55926a908e20 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:160:160 [3] NCCL INFO comm 0x55f33bd85610 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:162:162 [4] NCCL INFO comm 0x557fdb0eee60 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:164:164 [6] NCCL INFO comm 0x55584759ebe0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:163:163 [5] NCCL INFO comm 0x5558075f63f0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:165:165 [7] NCCL INFO comm 0x55fd7a143720 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-22 14:05:47.077 algo-1:158 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-22 14:05:47.077 algo-1:160 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-22 14:05:47.077 algo-1:164 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-22 14:05:47.077 algo-1:163 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-22 14:05:47.077 algo-1:156 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-22 14:05:47.077 algo-1:154 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-22 14:05:47.077 algo-1:162 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-22 14:05:47.086 algo-1:165 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-22 14:05:47.152 algo-1:162 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-22 14:05:47.152 algo-1:156 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-22 14:05:47.152 algo-1:154 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-22 14:05:47.152 algo-1:160 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-22 14:05:47.152 algo-1:164 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-22 14:05:47.152 algo-1:163 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-22 14:05:47.152 algo-1:165 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-22 14:05:47.152 algo-1:158 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-22 14:05:50.734 algo-1:154 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-22 14:05:50.734 algo-1:154 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-22 14:05:50.740 algo-1:165 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-22 14:05:50.741 algo-1:165 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-22 14:05:50.748 algo-1:156 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-22 14:05:50.749 algo-1:156 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-22 14:05:50.778 algo-1:160 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-22 14:05:50.779 algo-1:160 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-22 14:05:50.926 algo-1:163 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-22 14:05:50.927 algo-1:163 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/co[1,4]<stdout>:[2021-02-22 14:05:50.940 algo-1:162 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-22 14:05:50.940 algo-1:162 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/co[1,2]<stdout>:[2021-02-22 14:05:50.938 algo-1:158 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-22 14:05:50.938 algo-1:158 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/co[1,3]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:[1,6]<stdout>:[2021-02-22 14:05:50.972 algo-1:164 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-22 14:05:50.972 algo-1:164 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 397, in <module>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    main2(args)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"cifar10_keras_sm_ddp_82.py\", line 313, in main2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    loss_value = training_step(images, labels, batch == 0)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    result = self._call(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    self._initialize(args, kwds, add_initializers_to=initializers)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    *args, **kwds))\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2857, in _get_concrete_function_internal_garbage_collected\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    graph_function, _, _ = self._maybe_define_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3215, in _maybe_define_function\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    graph_function = self._create_graph_function(args, kwargs)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3077, in _create_graph_function\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    capture_by_value=self._capture_by_value),\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    func_outputs = python_func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    return weak_wrapped_fn().__wrapped__(*args, **kwds)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    raise e.ag_error_metadata.to_exception(e)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:AttributeError: in user code:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    cifar10_keras_sm_ddp_82.py:293 training_step  *\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        with tf.GradientTape() as tape:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:855 __enter__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        smdebug_hook = get_smdebug_hook(hook_type=\"keras\", create_if_not_exists=True)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/smdebug.py:122 get_smdebug_hook\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        return smd.get_hook(hook_type=hook_type, create_if_not_exists=create_if_not_exists)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/singleton_utils.py:41 get_hook\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        create_if_not_exists=create_if_not_exists,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:60 get_hook\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        _create_hook(json_config_path, hook_class)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/singleton_utils.py:28 _create_hook\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        hook = hook_class.create_from_json_file(json_file_path=json_config_path)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:309 create_from_json_file\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        return create_hook_from_json_config(cls, json_config_path=json_file_path)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/co[1,5]<stdout>:re/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:re/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:83 __init__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:re/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:re/json_config.py:244 create_hook_from_json_config\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        save_all=save_all,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/keras.py:93 __init__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/base_hook.py:83 __init__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        profiler_config_parser=profiler_config_parser,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/hook.py:238 __init__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        self.timeline_writer = TimelineFileWriter(profiler_config_parser=profiler_config_parser)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:138 __init__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        suffix=suffix,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/tfevent/timeline_file_writer.py:232 __init__\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        self.node_id = get_node_id()\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:393 get_node_id\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        rank = get_distributed_worker()\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    /usr/local/lib/python3.7/site-packages/smdebug/core/utils.py:373 get_distributed_worker\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:        if _smdataparallel_imported.get_world_size():\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    AttributeError: module 'smdistributed.dataparallel.tensorflow' has no attribute 'get_world_size'\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-02-22 14:05:36.988027: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-02-22 14:05:36.988225: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-02-22 14:05:36.988377: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-02-22 14:05:36.988518: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-02-22 14:05:37.007200: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-02-22 14:05:37.007202: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-02-22 14:05:37.007430: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-02-22 14:05:37.007430: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-02-22 14:05:37.029600: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-02-22 14:05:37.029772: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-02-22 14:05:37.030806: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-02-22 14:05:37.030914: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-02-22 14:05:37.049473: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-02-22 14:05:37.049665: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-02-22 14:05:37.071120: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-02-22 14:05:37.086655: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-02-22 14:05:37.086654: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-02-22 14:05:37.086838: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-02-22 14:05:37.086838: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-02-22 14:05:37.106264: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-02-22 14:05:37.106435: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-02-22 14:05:37.128852: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-02-22 14:05:37.129674: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-02-22 14:05:37.147625: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 0 in communicator MPI COMMUNICATOR 5 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\n",
      "\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[ip-10-0-79-111.ap-northeast-2.compute.internal:00146] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[ip-10-0-79-111.ap-northeast-2.compute.internal:00146] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "\u001b[0m\n",
      "\u001b[34m2021-02-22 14:05:51,047 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so smddprun /usr/local/bin/python3.7 -m mpi4py cifar10_keras_sm_ddp_82.py --model_dir s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-02-22 14:05:36.988027: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-02-22 14:05:36.988225: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-02-22 14:05:36.988377: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-02-22 14:05:36.988518: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-02-22 14:05:37.007200: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-02-22 14:05:37.007202: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-02-22 14:05:37.007430: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-02-22 14:05:37.007430: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-02-22 14:05:37.029600: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-02-22 14:05:37.029772: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-02-22 14:05:37.030806: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-02-22 14:05:37.030914: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-02-22 14:05:37.049473: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-02-22 14:05:37.049665: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-02-22 14:05:37.071120: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-02-22 14:05:37.086655: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-02-22 14:05:37.086654: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-02-22 14:05:37.086838: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-02-22 14:05:37.086838: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-02-22 14:05:37.106264: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-02-22 14:05:37.106435: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-02-22 14:05:37.128852: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-02-22 14:05:37.129674: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-02-22 14:05:37.147625: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 0 in communicator MPI COMMUNICATOR 5 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\n",
      "\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[ip-10-0-79-111.ap-northeast-2.compute.internal:00146] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[ip-10-0-79-111.ap-northeast-2.compute.internal:00146] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\n",
      "2021-02-22 14:06:14 Uploading - Uploading generated training model\n",
      "2021-02-22 14:06:14 Failed - Training job failed\n",
      "ProfilerReport-1614002353: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job tf-cifar10-ddp-2021-02-22-13-59-13-709: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so smddprun /usr/local/bin/python3.7 -m mpi4py cifar10_keras_sm_ddp_82.py --model_dir s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"\n[1,5]<stderr>:2021-02-22 14:05:36.988027: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n[1,5]<stderr>:2021-02-22 14:05:36.988225: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dd7cda6681bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m pt_estimator.fit({'train':'{}/train'.format(dataset_location),\n\u001b[1;32m     22\u001b[0m               \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'{}/validation'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m               'eval':'{}/eval'.format(dataset_location)})\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3640\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3642\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3220\u001b[0m                 ),\n\u001b[1;32m   3221\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m             )\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job tf-cifar10-ddp-2021-02-22-13-59-13-709: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so smddprun /usr/local/bin/python3.7 -m mpi4py cifar10_keras_sm_ddp_82.py --model_dir s3://sagemaker-ap-northeast-2-057716757052/tf-cifar10-ddp-2021-02-22-13-59-13-709/model\"\n[1,5]<stderr>:2021-02-22 14:05:36.988027: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n[1,5]<stderr>:2021-02-22 14:05:36.988225: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is "
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "pt_estimator = TensorFlow(\n",
    "                        base_job_name='tf-cifar10-ddp',\n",
    "                        source_dir='training_script',    \n",
    "                        entry_point='cifar10_keras_sm_ddp_82.py',\n",
    "                        role=role,\n",
    "                        py_version='py37',\n",
    "                        framework_version='2.3.1',\n",
    "                        # For training with multinode distributed training, set this count. Example: 2\n",
    "                        instance_count=1,\n",
    "                        # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "                        instance_type= 'ml.p3.16xlarge',\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        # Training using SMDataParallel Distributed Training Framework\n",
    "                        distribution={'smdistributed':{\n",
    "                                            'dataparallel':{\n",
    "                                                    'enabled': True\n",
    "                                             }\n",
    "                                      }}\n",
    "                        )\n",
    "pt_estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
